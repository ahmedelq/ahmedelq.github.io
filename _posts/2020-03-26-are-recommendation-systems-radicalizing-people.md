---
layout: post
title: Are recommendation systems radicalizing people?
subtitle : or maybe not
author: Ahmed Alq
tags: [opinions]
comments: true
---
![Bubbles (Photo by David Clode on Unsplash)](https://miro.medium.com/max/1200/0*t4b-h8wdYtUu93Ap)

Let’s say you’re running an online shopping business, hundreds of items are listed, yet your customers just search for the products they need before they buy it and leave. You notice this pattern and think, how can I maximize my profits further? One way is perhaps by showing the top selling items or even by random selection. But because you’re smart you decide to make use of the customer's purchase history, in-cart items, search terms, and perhaps even region, gender or other metadata and compare it with other data you have to make item suggestions more specific to guarantee more hit rates. <br/><br/>



This is essentially what a recommender system is, it feeds on millions of data records of traced users behavior and online history to produce a prediction of the most suitable item out of millions of items in order to keep the user satisfied or engaged as long as possible; it solves the problem of finding a needle in a haystack. <br/><br/>

That’s why it is used ubiquitously in the online world, YouTube, Netflix, and Spotify for example are all driven by a recommender algorithm. Usually it is a win-win situation; the company increases its utility by getting the [user’s attention](https://sloanreview.mit.edu/article/how-we-sell-our-attention/), and users satisfy their needs and are relieved from the burdens of long searches, especially when they don’t know what they want.<br/><br/>

## Double-edged sword
But what are the consequences of this approach? What would happen if you started watching more videos about vegan dishes for example? The YouTube recommender algorithm will notice that and will try to keep you engaged and satisfied as long as possible. Soon most of the suggested videos would be related to veganism, it created a [bubble](https://www.youtube.com/watch?v=B8ofWFx525s) and isolated you from seeing pro-carnivore videos. Thus begins your journey towards Veganism radicalization. 
Now imagine instead of veganism, it was about radical content, Xenophobia for example. This would [arguably](https://www.nytimes.com/interactive/2019/06/08/technology/youtube-radical.html) make you more radical, wouldn’t it? Well, Maybe, But i think a wise implementation of a recommender system is the best solution to mitigate or counter radicalism, and why it is more important to have it than not. <br/><br/>


To better understand the complexity of the subject, we must first analyze the psychology of the user, [uses and gratifications theory](https://en.wikipedia.org/wiki/Uses_and_gratifications_theory) (UGT) argues that users choose a particular source (e.g YouTube) in hope to satisfy a particular need: entertainment, debating, validation, etc. And because the relevance of user’s needs are influenced by the culture, personal choices, environment among many other factors that affects the user. The aim of a good recommendation algorithm would be to accord the presented content with these influencing factors to satisfy the user's needs. Otherwise, according to UGT users will change the source. Thus disabling the recommendation system, will force some users to change to another unregulated source and featuring more intense and radical content, perhaps on the dark web.<br/><br/>
Just as with [supervised drug consumption facilities](https://en.wikipedia.org/wiki/Supervised_injection_site), online platforms must act the same way; providing supervised content that satisfies the user albeit of little extremism or bias. This is an even more effective solution to combat radicalization; the community being large and diverse by different ideologies and motivated by virtue signaling to achieve justice, will criticize and create awareness about extremism, a good example is the [infamous case of Justine Sacco](https://en.wikipedia.org/wiki/Online_shaming#Justine_Sacco_incident).<br/><br/>


An effective policy is a key component in a regulated platform to prevent the middle and upper tiers of extremism, which must be supported by governments and organizations to allow building healthy or semi-healthy discussion platforms. Other lower-tiers of extremism should be left out for the reasons mentioned above. <br/><br/>

To prevent filter bubbles, one possible solution would be to present content by topic as well as by a specific metadata. For example if a vegan engages in a topic about cooking, subjects about cooking in general and about vegan dishes must be recommended. Thus bubble would be penetrated. Another approach is to integrate the recommender algorithm with an [AI-based Socratic discussion enhancement](https://link.springer.com/article/10.1007/s12152-019-09401-y), which aims to change users’ behavior by revealing fallacies in their thinking.<br/><br/>

## Education, the first and last frontier

Yet a more effective approach lies in education; being prone to radical ideologies is inevitable and could be encountered anywhere regardless of the carrier medium whether it is in the web or TV. Hence, just as with tobacco control campaigns, governments must be obligated to instill public awareness about racism. Prior to Internet usage, schools must equip students with the necessary mental tools such as offering logical and critical thinking courses, and let them question extreme proclaims in a safe environment to have a more resilient mindset.<br/><br/>

Also published on [Medium](https://medium.com/@ahmelq/are-recommendation-systems-radicalizing-people-c015c0fbad1f) <br/><br/>
